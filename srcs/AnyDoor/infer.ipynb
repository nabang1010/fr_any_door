{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/anydoor/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded model config from [configs/anydoor.yaml]\n",
      "Loaded state_dict from [pretrained/epoch=1-step=8687.ckpt]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from pytorch_lightning import seed_everything\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "from cldm.hack import disable_verbosity, enable_sliced_attention\n",
    "from datasets.data_utils import * \n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "import albumentations as A\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "save_memory = False\n",
    "disable_verbosity()\n",
    "if save_memory:\n",
    "    enable_sliced_attention()\n",
    "\n",
    "\n",
    "config = OmegaConf.load('./configs/inference.yaml')\n",
    "model_ckpt =  config.pretrained_model\n",
    "model_config = config.config_file\n",
    "\n",
    "model = create_model(model_config ).cpu()\n",
    "model.load_state_dict(load_state_dict(model_ckpt, location='cuda'))\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "\n",
    "\n",
    "def aug_data_mask(image, mask):\n",
    "    transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        ])\n",
    "    transformed = transform(image=image.astype(np.uint8), mask = mask)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    transformed_mask = transformed[\"mask\"]\n",
    "    return transformed_image, transformed_mask\n",
    "\n",
    "\n",
    "def process_pairs(ref_image, ref_mask, tar_image, tar_mask):\n",
    "    # ========= Reference ===========\n",
    "    # ref expand \n",
    "    ref_box_yyxx = get_bbox_from_mask(ref_mask)\n",
    "\n",
    "    # ref filter mask \n",
    "    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n",
    "    print(\"ref_image.shape\", ref_image.shape)\n",
    "    print(\"ref_mask_3.shape\", ref_mask_3.shape)\n",
    "    \n",
    "    masked_ref_image = ref_image * ref_mask_3 + np.ones_like(ref_image) * 255 * (1-ref_mask_3)\n",
    "\n",
    "    y1,y2,x1,x2 = ref_box_yyxx\n",
    "    masked_ref_image = masked_ref_image[y1:y2,x1:x2,:]\n",
    "    ref_mask = ref_mask[y1:y2,x1:x2]\n",
    "\n",
    "\n",
    "    ratio = np.random.randint(12, 13) / 10\n",
    "    masked_ref_image, ref_mask = expand_image_mask(masked_ref_image, ref_mask, ratio=ratio)\n",
    "    ref_mask_3 = np.stack([ref_mask,ref_mask,ref_mask],-1)\n",
    "\n",
    "    # to square and resize\n",
    "    masked_ref_image = pad_to_square(masked_ref_image, pad_value = 255, random = False)\n",
    "    masked_ref_image = cv2.resize(masked_ref_image, (224,224) ).astype(np.uint8)\n",
    "\n",
    "    ref_mask_3 = pad_to_square(ref_mask_3 * 255, pad_value = 0, random = False)\n",
    "    ref_mask_3 = cv2.resize(ref_mask_3, (224,224) ).astype(np.uint8)\n",
    "    ref_mask = ref_mask_3[:,:,0]\n",
    "\n",
    "    # ref aug \n",
    "    masked_ref_image_aug = masked_ref_image #aug_data(masked_ref_image) \n",
    "\n",
    "    # collage aug \n",
    "    masked_ref_image_compose, ref_mask_compose = masked_ref_image, ref_mask #aug_data_mask(masked_ref_image, ref_mask) \n",
    "    masked_ref_image_aug = masked_ref_image_compose.copy()\n",
    "    ref_mask_3 = np.stack([ref_mask_compose,ref_mask_compose,ref_mask_compose],-1)\n",
    "    ref_image_collage = sobel(masked_ref_image_compose, ref_mask_compose/255)\n",
    "\n",
    "    # ========= Target ===========\n",
    "    tar_box_yyxx = get_bbox_from_mask(tar_mask)\n",
    "    tar_box_yyxx = expand_bbox(tar_mask, tar_box_yyxx, ratio=[1.1,1.2])\n",
    "\n",
    "    # crop\n",
    "    tar_box_yyxx_crop =  expand_bbox(tar_image, tar_box_yyxx, ratio=[1.5, 3])    #1.2 1.6\n",
    "    tar_box_yyxx_crop = box2squre(tar_image, tar_box_yyxx_crop) # crop box\n",
    "    y1,y2,x1,x2 = tar_box_yyxx_crop\n",
    "\n",
    "    cropped_target_image = tar_image[y1:y2,x1:x2,:]\n",
    "    tar_box_yyxx = box_in_box(tar_box_yyxx, tar_box_yyxx_crop)\n",
    "    y1,y2,x1,x2 = tar_box_yyxx\n",
    "\n",
    "    # collage\n",
    "    ref_image_collage = cv2.resize(ref_image_collage, (x2-x1, y2-y1))\n",
    "    ref_mask_compose = cv2.resize(ref_mask_compose.astype(np.uint8), (x2-x1, y2-y1))\n",
    "    ref_mask_compose = (ref_mask_compose > 128).astype(np.uint8)\n",
    "\n",
    "    collage = cropped_target_image.copy() \n",
    "    collage[y1:y2,x1:x2,:] = ref_image_collage\n",
    "\n",
    "    collage_mask = cropped_target_image.copy() * 0.0\n",
    "    collage_mask[y1:y2,x1:x2,:] = 1.0\n",
    "\n",
    "    # the size before pad\n",
    "    H1, W1 = collage.shape[0], collage.shape[1]\n",
    "    cropped_target_image = pad_to_square(cropped_target_image, pad_value = 0, random = False).astype(np.uint8)\n",
    "    collage = pad_to_square(collage, pad_value = 0, random = False).astype(np.uint8)\n",
    "    collage_mask = pad_to_square(collage_mask, pad_value = -1, random = False).astype(np.uint8)\n",
    "\n",
    "    # the size after pad\n",
    "    H2, W2 = collage.shape[0], collage.shape[1]\n",
    "    cropped_target_image = cv2.resize(cropped_target_image, (512,512)).astype(np.float32)\n",
    "    collage = cv2.resize(collage, (512,512)).astype(np.float32)\n",
    "    collage_mask  = (cv2.resize(collage_mask, (512,512)).astype(np.float32) > 0.5).astype(np.float32)\n",
    "\n",
    "    masked_ref_image_aug = masked_ref_image_aug  / 255 \n",
    "    cropped_target_image = cropped_target_image / 127.5 - 1.0\n",
    "    collage = collage / 127.5 - 1.0 \n",
    "    collage = np.concatenate([collage, collage_mask[:,:,:1]  ] , -1)\n",
    "\n",
    "    item = dict(ref=masked_ref_image_aug.copy(), jpg=cropped_target_image.copy(), hint=collage.copy(), extra_sizes=np.array([H1, W1, H2, W2]), tar_box_yyxx_crop=np.array( tar_box_yyxx_crop ) ) \n",
    "    return item\n",
    "\n",
    "\n",
    "def crop_back( pred, tar_image,  extra_sizes, tar_box_yyxx_crop):\n",
    "    H1, W1, H2, W2 = extra_sizes\n",
    "    y1,y2,x1,x2 = tar_box_yyxx_crop    \n",
    "    pred = cv2.resize(pred, (W2, H2))\n",
    "    m = 5 # maigin_pixel\n",
    "\n",
    "    if W1 == H1:\n",
    "        tar_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n",
    "        return tar_image\n",
    "\n",
    "    if W1 < W2:\n",
    "        pad1 = int((W2 - W1) / 2)\n",
    "        pad2 = W2 - W1 - pad1\n",
    "        pred = pred[:,pad1: -pad2, :]\n",
    "    else:\n",
    "        pad1 = int((H2 - H1) / 2)\n",
    "        pad2 = H2 - H1 - pad1\n",
    "        pred = pred[pad1: -pad2, :, :]\n",
    "\n",
    "    gen_image = tar_image.copy()\n",
    "    gen_image[y1+m :y2-m, x1+m:x2-m, :] =  pred[m:-m, m:-m]\n",
    "    return gen_image\n",
    "\n",
    "\n",
    "def inference_single_image(ref_image, ref_mask, tar_image, tar_mask, guidance_scale = 5.0):\n",
    "    item = process_pairs(ref_image, ref_mask, tar_image, tar_mask)\n",
    "    ref = item['ref'] * 255\n",
    "    tar = item['jpg'] * 127.5 + 127.5\n",
    "    hint = item['hint'] * 127.5 + 127.5\n",
    "\n",
    "    hint_image = hint[:,:,:-1]\n",
    "    hint_mask = item['hint'][:,:,-1] * 255\n",
    "    hint_mask = np.stack([hint_mask,hint_mask,hint_mask],-1)\n",
    "    ref = cv2.resize(ref.astype(np.uint8), (512,512))\n",
    "\n",
    "    seed = random.randint(0, 65535)\n",
    "    if save_memory:\n",
    "        model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "    ref = item['ref']\n",
    "    tar = item['jpg'] \n",
    "    hint = item['hint']\n",
    "    num_samples = 1\n",
    "\n",
    "    control = torch.from_numpy(hint.copy()).float().cuda() \n",
    "    control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "    control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "\n",
    "    clip_input = torch.from_numpy(ref.copy()).float().cuda() \n",
    "    clip_input = torch.stack([clip_input for _ in range(num_samples)], dim=0)\n",
    "    clip_input = einops.rearrange(clip_input, 'b h w c -> b c h w').clone()\n",
    "\n",
    "    guess_mode = False\n",
    "    H,W = 512,512\n",
    "\n",
    "    cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning( clip_input )]}\n",
    "    un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([torch.zeros((1,3,224,224))] * num_samples)]}\n",
    "    shape = (4, H // 8, W // 8)\n",
    "\n",
    "    if save_memory:\n",
    "        model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "    # ====\n",
    "    num_samples = 1 #gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
    "    image_resolution = 512  #gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n",
    "    strength = 1  #gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n",
    "    guess_mode = False #gr.Checkbox(label='Guess Mode', value=False)\n",
    "    #detect_resolution = 512  #gr.Slider(label=\"Segmentation Resolution\", minimum=128, maximum=1024, value=512, step=1)\n",
    "    ddim_steps = 50 #gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
    "    scale = guidance_scale  #gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n",
    "    seed = -1  #gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n",
    "    eta = 0.0 #gr.Number(label=\"eta (DDIM)\", value=0.0)\n",
    "\n",
    "    model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "    samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
    "                                                    shape, cond, verbose=False, eta=eta,\n",
    "                                                    unconditional_guidance_scale=scale,\n",
    "                                                    unconditional_conditioning=un_cond)\n",
    "    if save_memory:\n",
    "        model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "    x_samples = model.decode_first_stage(samples)\n",
    "    x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy()#.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    result = x_samples[0][:,:,::-1]\n",
    "    result = np.clip(result,0,255)\n",
    "\n",
    "    pred = x_samples[0]\n",
    "    pred = np.clip(pred,0,255)[1:,:,:]\n",
    "    sizes = item['extra_sizes']\n",
    "    tar_box_yyxx_crop = item['tar_box_yyxx_crop'] \n",
    "    gen_image = crop_back(pred, tar_image, sizes, tar_box_yyxx_crop) \n",
    "    return gen_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_image.shape (1024, 768, 3)\n",
      "ref_mask_3.shape (1024, 768, 3)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:11<00:00,  4.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_image = cv2.imread('/home/xuananh/work/nabang1010/fr_any_door/test/cloth/demo_5.jpg')\n",
    "ref_image = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "gt_image = cv2.imread(\"/home/xuananh/work/nabang1010/fr_any_door/test/image/demo_5.jpg\")\n",
    "gt_image = cv2.resize(gt_image, (ref_image.shape[1], ref_image.shape[0]))\n",
    "gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "ref_mask = (cv2.imread(\"/home/xuananh/work/nabang1010/fr_any_door/test/cloth-mask/demo_5.jpg\") > 128).astype(np.uint8)[:,:,0]\n",
    "\n",
    "\n",
    "tar_mask = Image.open(\"/home/xuananh/work/nabang1010/fr_any_door/test/image-parse-v3/demo_5.png\")\n",
    "\n",
    "tar_mask= np.array(tar_mask)\n",
    "tar_mask = cv2.resize(tar_mask, (ref_image.shape[1], ref_image.shape[0]))\n",
    "gen_image = inference_single_image(ref_image, ref_mask, gt_image.copy(), tar_mask)\n",
    "\n",
    "cv2.imwrite(\"/home/xuananh/work/nabang1010/fr_any_door/srcs/AnyDoor/VITONGEN/demo_5.jpg\", gen_image[:,:,::-1])\n",
    "\n",
    "gen_path = \"/home/xuananh/work/nabang1010/fr_any_door/srcs/AnyDoor/VITONGEN/demo_5_full.jpg\"\n",
    "vis_image = cv2.hconcat([ref_image, gt_image, gen_image])\n",
    "cv2.imwrite(gen_path, vis_image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anydoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
